---
layout: post
title: Deep Learning Basics - RNN의 발전과 Transformer
category: BC
tag: [Deep Learning] 
use_math: true
---

## Sequential Model

- Naive sequence model  
입력이 들어왔을 때 이전까지 들어왔던 데이터를 기반으로 다음 데이터를 예측하는 것이다.  

- Autoregressive model  
$\tau$ 라는 변수를 추가한다. Moving window 개념이며 과거 중 $\tau$ 만큼의 크기만 본다는 개념이다.  

- Markov model  
    $$
    p\left(x_{1}, \ldots, x_{T}\right)=p\left(x_{T} \mid x_{T-1}\right) p\left(x_{T-1} \mid x_{T-2}\right) \cdots p\left(x_{2} \mid x_{1}\right) p\left(x_{1}\right)=\prod_{t=1}^{T} p\left(x_{t} \mid x_{t-1}\right)
    $$
    직전의 과거의 정보에 대하여 조건부 연산을 한다. 하지만 직전의 과거만으로는 부족한 정보가 너무 많다.  

- Latent autoregressive model  
    과거의 정보를 요약해주는 **Hidden state**를 생성
    $$
    \begin{aligned}
    \hat{x} &=p\left(x_{t} \mid h_{t}\right) \\
    h_{t} &=q\left(h_{t-1}, x_{t-1}\right.
    \end{aligned}
    $$  

## RNN  

과거의 정보를 바탕으로 입력에 대한 출력을 뽑을 수 있는 네트워크이다. 이전의 정보는 지속적으로 Fully connect layer로 연결된다.  

- Short-term dependencies

    최근의 과거에 많은 의존성을 가지게 된다는 점이다.  

- Long-term dependencies  

    최근에 과거에 비해, 오래된 과거는 의존성이 많이 떨어지게 된다. 과연 최근의 과거로만 올바른 추론을 할 수 있을까?  

#### Long Short Term Memory  

기존의 RNN은 하나의 입력으로 한 레이어에서 한번의 활성함수를 거친 뒤 출력을 뽑아내는 구조이다. 
하지만 LSTM은 개선됐다. 핵심 아이디어는, 좋은 정보인지 결정을 하고, 좋은 정보만 올려준다는 **게이트**의 개념이다.   
LSTM에서는 Previous cell state, Previous hidden state, Input 이렇게 3가지 입력이 들어온다. 다음 출력도 Output, Next cell state, Next hidden state 3가지가 있다.  

#### GRU  
간단하게 입력과 출력이 2개로 LSTM보다 1개씩 적다.  
LSTM보다 성능이 좋을 때가 많다.  

## Transformer  

RNN으로 문제를 해결할 때, 만약 정보가 순차적으로 들어오는게 보장되지 않는 상황이라면 문제를 해결하기 어려울 때가 있다.  
그렇게 고민을 하다 **Transformer**라는 현재 상당히 좋은 결과를 보이는 모델이 개발됐으며, 특징으로는 NLP에서 자주 쓰이지만 Encoder-Decoder 방식이라 재귀적인 연산이 없다. 최근에는 CV 분야에서도 상당히 많이 연구되고있다.  

번역에서 한 상황을 생각해보면, 입력과 출력의 단어가 다를 수도 있다. 영어로는 3문장으로 나오는게 한국어로는 1문장으로 나올수도 있는듯이.  

내용을 풀어쓰기엔 너무 많아 기본적인 개념만 써보자면 한 단어의 뜻을 결정할 때, 다른 단어들의 정보를 참고하여 각각에 대한 weight를 설정하는 **Attention** 이라는 핵심 기능을 사용한다.  

최근 이를 CV분야에서도 사용하는 Vision Transformer, Generator 개념으로 사용하는 DALL-E등의 연구가 있었다.  
 

