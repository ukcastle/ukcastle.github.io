---
layout: post
title: Deep Learning Basics - RNN의 발전
category: BC
tag: [Deep Learning] 
use_math: true
---

## Sequential Model

- Naive sequence model  
입력이 들어왔을 때 이전까지 들어왔던 데이터를 기반으로 다음 데이터를 예측하는 것이다.  

- Autoregressive model  
$\tau$ 라는 변수를 추가한다. Moving window 개념이며 과거 중 $\tau$ 만큼의 크기만 본다는 개념이다.  

- Markov model  
    $$
    p\left(x_{1}, \ldots, x_{T}\right)=p\left(x_{T} \mid x_{T-1}\right) p\left(x_{T-1} \mid x_{T-2}\right) \cdots p\left(x_{2} \mid x_{1}\right) p\left(x_{1}\right)=\prod_{t=1}^{T} p\left(x_{t} \mid x_{t-1}\right)
    $$
    직전의 과거의 정보에 대하여 조건부 연산을 한다. 하지만 직전의 과거만으로는 부족한 정보가 너무 많다.  

- Latent autoregressive model  
    과거의 정보를 요약해주는 **Hidden state**를 생성
    $$
    \begin{aligned}
    \hat{x} &=p\left(x_{t} \mid h_{t}\right) \\
    h_{t} &=q\left(h_{t-1}, x_{t-1}\right.
    \end{aligned}
    $$  

## RNN  

과거의 정보를 바탕으로 입력에 대한 출력을 뽑을 수 있는 네트워크이다. 이전의 정보는 지속적으로 Fully connect layer로 연결된다.  

- Short-term dependencies
