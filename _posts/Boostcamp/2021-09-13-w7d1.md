---
layout: post
title: Computer Vision - CNN Visualization
category: BC
tag: [Deep Learning]
use_math: true
---

## Further Question  
- 왜 Filter visualization에서 주로 첫번째 Convolution layer를 목표로 하는가?
- Occlusion map에서 heatmap이 의미하는 바가 무엇인가?  
- Grad-CAM에서 Linear combination의 결과를 ReLU Layer에 거치는 이유는?  


## CNN은 어떤 방식으로 물체를 인식할까?  

만약 모델이 원하는대로 학습이 잘 안된다면, 어떻게 알 수 있을까?  
바로 시각화를 통해 디버깅을 할 수 있다.  

#### Filter Visualization  

- 왜 첫 번째 레이어의 시각화를 하는가?  
    고차원의 레이어로 갈수록 차원 수가 매우 높다보니 사람의 눈으로 해석하기엔 상당히 무리가 있다. 따라서 첫 번째 레이어의 해석을 목표로 한다.  

따라서 Filter Visualization과 같은 Low-Level 방법은 **모델 자체의 특성을 분석**하는 방법론으로 분류된다.  

#### Nearest Neighbors

CNN의 마지막 레이어인 fc layer을 잘라낸 뒤 이전 레이어에 집중한다.    
특정한 고차원의 output이 출력될텐데, 어떤 input을 넣어 그에 맞는 output을 확인하고, 그것을 여러번 반복하여 주변에 비슷한 input들이 있는지 확인하는 방법이다.  

#### Dimensionality reduction  

t-SNE와 같은 방법이 있는데, 간단히 Nearest Neighbors를 시각화 가능한 2~3차원으로 매핑하는 방법이다.  

#### Activation Investigation  

- Layer Activation: mid-high level 사이에서 시각화하는 방법이다. 모델이 어떤 부분에 주목하고 있는지를 시각화해준다.  
- Maximally Activationg Patches: mid level에서 시각화하는 방법으로 모델에서 특정한 레이어를 지목했을 때 레이어가 집중하고 있는 국부적인 패치부분만 시각화하여 보여준다.  
- Class Visualization: 매 iter이 흘러갈 때 마다 모델이 집중하고 있는 부분을 보여준다.  

##### Class Visualization  

하지만 간단하게 시각화할 순 없고, 일단 Loss를 설정해주어야 한다.  

$$
I^{*}=\arg \max f(I)-\operatorname{Reg}(I)
$$  

I에 대한 output이 최대가 되는 값을 찾는다. 다만 너무 커지면 영상이 아닌 값이 나올수도 있어 Regularization을 한다. $\lambda\|I\|_{2}^{2}$와 같은 식으로 일반화해준다.  

이 때 **Gradient Ascent**방식으로 진행이 되는데, Loss를 Backpropagation을 한 뒤 **입력 영상을 Update 해주고** 다시 Loss를 줄이는 방향으로 Gradient Ascent를 하는 방식으로 진행이 된다.  

#### Occlusion Map  

이미지에 랜덤한 Occlusion Map을, 즉 마스크로 특정 부분을 가린 뒤 Class의 Score를 보고 Score의 변화량을 보며, Score가 급감하면 가린 부분에 Heatmap을 표현하는 방법이다.  

#### Backpropagation-based Saliency  

기존의 Backpropagation을 구할 땐 $\frac{\partial L}{\partial h^{l}}=\left[\left(h^{l+1}>0\right)\right] \frac{\partial L}{\partial h^{l+1}}$ 와 같은 식을 사용한다.  

하지만 수학적으론 좀 이상하지만, 시각화를 할 땐 ReLU를 적용하고 상위 Layer를 참조하는 $\frac{\partial L}{\partial h^{l}}=\left[\left(h^{l}>0\right) \&\left(h^{l+1} \geq 0\right)\right] \frac{\partial L}{\partial h^{l+1}}$ 와 같은 **Guided Backpropagation**을 사용한다.  

왜 그렇게 하냐면, Gradient와는 별 상관이 없고 Backprop과 DeConv에는 서로 약간 다른 결과값을 보이는데, 이를 And로 묶어주면 상당히 강력한 시각화방법이 된다는 것이 보인다.  

#### CAM(Class Activation Mapping)  

Neural Network를 조금 수정해줘야되는데, FC Layer에 가기 전에 Global Average Pooling을 거치고 한번의 fc layer만 거치도록 바꿔줘야한다.  

GAP이란 각 채널마다의 평균값을 구해주는 방법이다.  

그렇게 결과값으로 나온 값을 보면 모델이 어느 위치를 집중하고있는지 알 수 있다.  

#### Grad-CAM  

CAM의 단점은 모델의 구조를 바꿔줘야 하고 특정한 구조만을 유지해야하는데, Grad-CAM의 경우는 기존의 모델을 변경하지 않고 시각화를 할 수 있다. 다만 Backbone이 CNN이기만 하면 된다는점이다.  

feature map에 $\alpha_{k}^{c}=\frac{1}{Z} \sum_{i} \sum_{j} \frac{\partial y^{c}}{\partial A_{i j}^{k}}$의 연산을 관심있는 Activation Map 까지만 진행하여 weight를 구하고 그 다음 선형 결합을 하고 ReLU를 적용해주는 $L_{G r a d-C A M}^{c}=\operatorname{Re} L U\left(\sum_{k} \alpha_{k}^{c} A^{k}\right)$을 통해 결과를 뽑아낸다.  

