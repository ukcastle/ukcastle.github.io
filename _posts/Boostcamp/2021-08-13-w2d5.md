---
layout: post
title: Deep Learning Basics - Generative Model
category: BC
tag: [Deep Learning] 
use_math: true
---

## Learning a Generative Model

- '강아지'의 이미지를 가지고있다 가정을 하자.  
- 다음과 같은 $p(x)$의 확률분포를 얻길 원한다.  
  1. Generation  
    만약 $x_{new} \sim p(x)$일 때, $x_{new}$ 는 강아지 처럼 보일것이다.  
    **Sampling**

  2. Density estimation  
    $p(x)$는 $x$가 강아지같다면 높아야하고 아니라면 낮아야한다.  
    **Anomaly detection**  
    엄밀히 말하자면 Generative Model은 분류 모델을 포함한다.   
  3. Unsupervised representation learning  
    여러 이미지들의 공통점이 무엇인지 배울 수 있어야 한다.  
    **Feature learning**  

- 그렇다면, 어떻게 $p(x)$를 나타낼 수 있을까?  

  베르누이 분포를 가지는, 즉 0과 1로만 이루어진 데이터라고 생각해보자.  
  모든 데이터가 독립이라고 했을 때, 데이터가 n개라면 총 파라미터의 수는 $2^n$ 개가 있다.  
  하지만 이미지의 측면에서 본다면 보통 주변의 픽셀과 비슷한 값을 가진다는 특징으로 볼 때 이는 말도 안되는 가정이라는 것을 알 수 있다.  

#### Conditional Independence  

$X_{i+1} \perp X_{1}, \ldots, X_{i-1} \mid X_{i}$ (Markov assumption)을 사용한다고 생각할 때, 파라미터의 수는 다음으로 정의된다.  
$$
p\left(x_{1}, \ldots, x_{n}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{2}\right) \cdots p\left(x_{n} \mid x_{n-1}\right)
$$  
이 때 파라미터의 수는 2n-1까지 줄일 수 있다.  
Auto Regressive Model은 이런 Conditional Independence를 이용한다.  