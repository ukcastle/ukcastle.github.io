---
layout: post
title: Ai Math - 통계학, 그리고 베이즈 통계학
category: BC
tag: [AiMath] 
use_math: true
---

## 통계학

- 통계적 모델링은 적절한 가정 위에서 확률분포를 추정하는 것
- 유한한 개수의 데이터만 관찰하여 모집단의 분포를 정확히 알아내는 것은 불가능하여, 확률분포를 **추정**함

#### 모수

- 평균과 분산
    $$
    \bar{X}=\frac{1}{N} \sum_{i=1}^{N} X_{i} \quad S^{2}=\frac{1}{N-1} \sum_{i=1}^{N}\left(X_{i}-\bar{X}\right)^{2}
    $$
- 데이터가 특정 확률분포를 따른다고 미리 가정을 한 후 그 분포를 결정하는 Parameter(모수)를 추정하는 것을 **모수적 방법론**
- 특정 확률분포를 가정하지 않고, 데이터에 따라 모델의 구조가 바뀌면 **비모수 방법론**

#### 확률분포
- 베르누이분포: 데이터가 2개의 값만 가지는 경우
- 카테고리분포: 데이터가 n개의 이산적인 값을 가지는 경우
- 베타분포: 데이터가 [0,1] 사이에서 값을 가지는 경우
- 감마분포, 로그정규분포: 데이터가 0 이상의 값을 가지는 경우
- 정규분포, 라플라스분포: 데이터가 R 전체에서 값을 가지는 경우

#### 최대 가능도 추정법(Maximum Likelihood Estimation)

$$
\hat{\theta}_{\mathrm{MLE}}=\underset{\theta}{\operatorname{argmax}} L(\theta ; \mathbf{x})=\underset{\theta}{\operatorname{argmax}} P(\mathbf{x} \mid \theta)
$$

- 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나인 MLE

- 데이터 집합 X가 독립적으로 추출되었을 경우 **로그가능도**를 최적화

    $$
    L(\theta ; \mathbf{X})=\prod_{i=1}^{n} P\left(\mathbf{x}_{i} \mid \theta\right) \quad \Rightarrow \quad \log L(\theta ; \mathbf{X})=\sum_{i=1}^{n} \log P\left(\mathbf{x}_{i} \mid \theta\right)
    $$

##### 로그가능도를 사용하는 이유

    - 데이터의 숫자가 수억단위가 되면 컴퓨터의 정확도로는 가능도를 계산하는 것이 어려움
    - 데이터가 독립일 경우 곱셉을 덧셈으로 바꿀 수 있음
    - 경사하강법으로 가능도를 추정할 때, O(n^2)에서 O(n)으로 줄일 수 있음
    - 대게의 손실함수의 경우 경사하강법을 사용하기 떄문에 음의 로그가능도를 최적화시킴

#### 딥러닝에서의 MLE

- 딥러닝 모델의 가중치를 $\theta=\left(\mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(L)}\right)$ 라 했을 때, 분류문제에서 소프트맥스 벡터는 카테고리분포의 모수 $\left(p_{1}, \ldots, p_{K}\right)$ 를 모델링
- One-Hot 벡터로 표현한 정답레이블 $\mathbf{y}=\left(y_{1}, \ldots, y_{K}\right)$ 를 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있음

    $$
    \hat{\theta}_{\mathrm{MLE}}=\underset{\theta}{\operatorname{argmax}} \frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i, k} \log \left(\operatorname{MLP}_{\theta}\left(\mathbf{x}_{i}\right)_{k}\right)
    $$

#### 확률분포의 거리
- 데이터 공간에서 P(x), Q(x)가 있을 경우, 두 확률분포 사이의 **거리**를 계산할 때 다음과 같은 방법을 이용

  1. 총 변동 거리
  2. 쿨백-라이블러 발산
  3. 바슈타인 거리

## 베이즈통계학

#### 조건부 확률

$$
\begin{gathered}
P(A \cap B)=P(B) P(A \mid B) \\
P(B \mid A)=\frac{P(A \cap B)}{P(A)}=P(B) \frac{P(A \mid B)}{P(A)}
\end{gathered}
$$

- 확률 B가 일어났을 때 사건 A가 발생할 확률

#### 베이즈 정리

$$
P(\theta \mid \mathcal{D})=P(\theta) \frac{P(\mathcal{D} \mid \theta)}{P(\mathcal{D})}
$$


- 조건부 확률을 이용하여 정보를 갱신하는 방법
- 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 **갱시된 사후확률**을 계산할 수 있음

#### 인과확률

- 조건부 확률은 유용한 통계적 해석을 제공하지만, 인과관계를 추론할 때 함부로 사용해선 안됨
- 인과관계는 데이터 분포의 변화에 **강건한 예측모형**을 만들 때 사용
- 인과관계를 알아내기 위해선 중첩요인의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 함

## 피어세션 정리

주요 키워드

 - 7강 통계학에서 카테고리 분포의 라그랑주 승수 계산에서 마지막 람다 처리

 - 가능도가 평균에 비해 가지는 이점 : 기계학습 특징상 모수를 모르더라도 추정 가능

 - 머신 러닝 관점에서의 통계학 : 

https://devkihyun.github.io/study/Machine-learining-and-Probability/

 - MLE 관련 자료 : 

https://angeloyeo.github.io/2020/07/17/MLE.html

 - 로그 가능도 계산 시 정확도 문제 : 극값의 위치가 같아 문제없다.

 - 쿨백 라이블러 발산이 음수가 될 가능성

 - 가능도는 반드시 모수의 분포를 가정해야하는가

     - 가정이 필수적이라면 무엇을 기준으로 가정하는지

     - 가정이 필수적이지 않다면 수식에서 P함수는 어떻게 처리되어야 하는지

## 회고