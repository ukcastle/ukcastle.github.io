---
layout: post
title: Ai Math - 딥러닝 학습방법과 확률과 통계
category: BC
tag: [AiMath] 
use_math: true
comments: true
---

## 딥러닝 학습방법

#### 신경망이란?

$$
\left[\begin{array}{c}
-\mathbf{o}_{1}- \\
-\mathbf{o}_{2} \\
\vdots \\
-\mathbf{o}_{n}
\end{array}\right]=\left[\begin{array}{c}
-\mathbf{x}_{1}- \\
-\mathbf{x}_{2}- \\
\vdots \\
-\mathbf{x}_{n}-
\end{array}\right]\left[\begin{array}{cccc}
w_{11} & w_{12} & \cdots & w_{1 p} \\
w_{21} & w_{22} & \cdots & w_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
w_{d 1} & w_{d 2} & \cdots & w_{d p}
\end{array}\right]+\left[\begin{array}{cccc}
\mid & & \cdots & \mid \\
b_{1} & b_{2} & \cdots & b_{p} \\
\mid & \mid & \cdots &
\end{array}\right]
$$

- 선형모델과 활성함수를 합성한 함수

- 출력벡터 o에 `softmax()` 함수를 합성하면 확률벡터가 특정 클래스 k에 속할 확률로 해석할 수 있다.

#### 활성함수  

- 비선형 함수의 일종으로 딥러닝에서 매우 중요한 개념이다  
- 활성함수를 쓰지 않으면, 딥러닝과 선형모형은 차이가 없다.  

$$
\sigma(x)=\frac{1}{1+e^{-x}} \quad \tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \quad \operatorname{ReLU}(x)=\max \{0, x\}
$$

위와 같이 **sigmoid, tanh, ReLU**를 많이 쓴다.  
특히 요즘날엔 ReLU를 가장 많이 쓴다

#### 소프트맥스

- 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산

$$
\operatorname{softmax}(\mathbf{o})=\left(\frac{\exp \left(o_{1}\right)}{\sum_{k=1}^{p} \exp \left(o_{k}\right)}, \ldots, \frac{\exp \left(o_{p}\right)}{\sum_{k=1}^{p} \exp \left(o_{k}\right)}\right)
$$

- 분류 문제를 풀 떈 선형함수와 소프트맥스 함수를 결합하여 예측한다  

- 추론을 할 땐 최대값을 가진 주소만 1로 만들고 나머지를 0으로 만드는 **One-Hot Vector**기법을 사용하여 Softmax 함수를 사용하지 않는다.  

#### 다층 신경망

- 이론상 2층 신경망으로도 함수를 근사할 수 있다.  
- 하지만 층이 깊을수록 목적함수를 근사할 때 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하다.  
    > 그렇다고 최적화가 쉽다는 뜻은 아니다.  

#### 순전파 

- 1층부터 결과까지 순차적으로 계산하는 것을 순전파라고 한다.   

$$ \begin{aligned}
&\mathbf{O}=\mathbf{Z}^{(L)} \\
&\mathbf{H}^{(\ell)}=\sigma\left(\mathbf{Z}^{(\ell)}\right)^{2}=1, \\
&\mathbf{Z}^{(\ell)}=\mathbf{H}^{(\ell-1)} \mathbf{W}^{(\ell)}+\mathbf{b}^{(\ell)} \\
&\vdots \\
&\mathbf{H}^{(1)}=\sigma\left(\mathbf{Z}^{(1)}\right) \\
&\mathbf{Z}^{(1)}=\mathbf{X} \mathbf{W}^{(1)}+\mathbf{b}^{(1)}
\end{aligned} $$

#### 역전파

- 딥러닝은 역전파 알고리즘을 사용해 각 층에 사용된 파라미터를 학습한다.  

$$
\left\{\mathbf{W}^{(\ell)}, \mathbf{b}^{(\ell)}\right\}_{\ell=1}^{L}
$$

- 순전파 알고리즘을 반대로 이용하는 것이다.  

- **연쇄법칙 기반 자동미분**을 사용한다.

$$\begin{equation}
\frac{\partial z}{\partial x}=\frac{\partial z}{\partial w} \frac{\partial w}{\partial x}
\end{equation}$$

## 확률

#### 딥러닝에서 확률이란

- 딥러닝은 기본적으로 확률론 기반의 기계학습
- 회귀 분석에서 손실함수인 L2-Norm은 예측오차의 분산을 가장 최소화하는 방향으로 학습
- 분류 분석에서 사용되는 교차엔트로피는 모델 예측의 불확실성을 최소화하는 방향으로 학습

즉, 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 한다.  

#### 이산확률변수

- 확률변수가 가질 수 있는 모든 경우의 수를 고려하여 확률을 더해 모델링한다.  
