---
layout: post
title: Ai Math - 딥러닝 학습방법과 확률
category: BC
tag: [AiMath] 
use_math: true
---

## 딥러닝 학습방법

#### 신경망이란?

$$
\left[\begin{array}{c}
-\mathbf{o}_{1}- \\
-\mathbf{o}_{2} \\
\vdots \\
-\mathbf{o}_{n}
\end{array}\right]=\left[\begin{array}{c}
-\mathbf{x}_{1}- \\
-\mathbf{x}_{2}- \\
\vdots \\
-\mathbf{x}_{n}-
\end{array}\right]\left[\begin{array}{cccc}
w_{11} & w_{12} & \cdots & w_{1 p} \\
w_{21} & w_{22} & \cdots & w_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
w_{d 1} & w_{d 2} & \cdots & w_{d p}
\end{array}\right]+\left[\begin{array}{cccc}
\mid & & \cdots & \mid \\
b_{1} & b_{2} & \cdots & b_{p} \\
\mid & \mid & \cdots &
\end{array}\right]
$$

- 선형모델과 활성함수를 합성한 함수

- 출력벡터 o에 `softmax()` 함수를 합성하면 확률벡터가 특정 클래스 k에 속할 확률로 해석할 수 있다.

#### 활성함수  

- 비선형 함수의 일종으로 딥러닝에서 매우 중요한 개념이다  
- 활성함수를 쓰지 않으면, 딥러닝과 선형모형은 차이가 없다.  

$$
\sigma(x)=\frac{1}{1+e^{-x}} \quad \tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \quad \operatorname{ReLU}(x)=\max \{0, x\}
$$

위와 같이 **sigmoid, tanh, ReLU**를 많이 쓴다.  
특히 요즘날엔 ReLU를 가장 많이 쓴다

#### 소프트맥스

- 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산

$$
\operatorname{softmax}(\mathbf{o})=\left(\frac{\exp \left(o_{1}\right)}{\sum_{k=1}^{p} \exp \left(o_{k}\right)}, \ldots, \frac{\exp \left(o_{p}\right)}{\sum_{k=1}^{p} \exp \left(o_{k}\right)}\right)
$$

- 분류 문제를 풀 떈 선형함수와 소프트맥스 함수를 결합하여 예측한다  

- 추론을 할 땐 최대값을 가진 주소만 1로 만들고 나머지를 0으로 만드는 **One-Hot Vector**기법을 사용하여 Softmax 함수를 사용하지 않는다.  

#### 다층 신경망

- 이론상 2층 신경망으로도 함수를 근사할 수 있다.  
- 하지만 층이 깊을수록 목적함수를 근사할 때 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하다.  
    > 그렇다고 최적화가 쉽다는 뜻은 아니다.  

#### 순전파 

- 1층부터 결과까지 순차적으로 계산하는 것을 순전파라고 한다.   

$$ \begin{aligned}
&\mathbf{O}=\mathbf{Z}^{(L)} \\
&\mathbf{H}^{(\ell)}=\sigma\left(\mathbf{Z}^{(\ell)}\right)^{2}=1, \\
&\mathbf{Z}^{(\ell)}=\mathbf{H}^{(\ell-1)} \mathbf{W}^{(\ell)}+\mathbf{b}^{(\ell)} \\
&\vdots \\
&\mathbf{H}^{(1)}=\sigma\left(\mathbf{Z}^{(1)}\right) \\
&\mathbf{Z}^{(1)}=\mathbf{X} \mathbf{W}^{(1)}+\mathbf{b}^{(1)}
\end{aligned} $$

#### 역전파

- 딥러닝은 역전파 알고리즘을 사용해 각 층에 사용된 파라미터를 학습한다.  

$$
\left\{\mathbf{W}^{(\ell)}, \mathbf{b}^{(\ell)}\right\}_{\ell=1}^{L}
$$

- 순전파 알고리즘을 반대로 이용하는 것이다.  

- **연쇄법칙 기반 자동미분**을 사용한다.

$$
\frac{\partial z}{\partial x}=\frac{\partial z}{\partial w} \frac{\partial w}{\partial x}
$$

## 확률

#### 딥러닝에서 확률이란

- 딥러닝은 기본적으로 확률론 기반의 기계학습
- 회귀 분석에서 손실함수인 L2-Norm은 예측오차의 분산을 가장 최소화하는 방향으로 학습
- 분류 분석에서 사용되는 교차엔트로피는 모델 예측의 불확실성을 최소화하는 방향으로 학습

즉, 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 한다.  

#### 확률변수

- 이산확률변수

    * 확률변수가 가질 수 있는 모든 경우의 수를 고려하여 확률을 더해 모델링한다.  

        $$
        \mathbb{P}(X \in A)=\sum_{\mathbf{x} \in A} P(X=\mathbf{x})
        $$  

- 연속확률변수

  * 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링한다.

    $$
    \mathbb{P}(X \in A)=\int_{A} P(\mathbf{x}) \mathrm{d} \mathbf{x}
    $$

- 확률변수는 데이터의 초상화

    * 데이터는 확률변수로 (x,y) ~ $\mathscr{D}$ 로 표기  
    * 결합분포 P(x,y)는 $\mathscr{D}$를 모델링
    * P(x)는 입력 x에 대한 **주변확률분포**로 y에 대한 정보를 주지 않음  

    $$
    P(\mathbf{x})=\sum_{v} P(\mathbf{x}, y) \quad P(\mathbf{x})=\int_{y} P(\mathbf{x}, y) \mathrm{d} y
    $$

    * **조건부 확률분포** P(x|y)는 데이터 공간에서 입력x와 출력 y 사이의 관계를 모델링

#### 조건부 확률

- P(y|x) 란, 입력변수 x에 대해 정답이 y일 확률
- 분류에서 softmax ($W\theta+b$)은 데이터 x로부터 추출된 특징패턴 $\theta(x)$과 가중치행렬 W를 통해 조건부확률 P(y|x)를 계산한다.
- 회귀문제의 경우 조건부 기대값 $\mathbb{E}_{y \sim P(y \mid \mathbf{x})}[y \mid \mathbf{x}]=\int_{y} y P(y \mid \mathbf{x}) \mathrm{d} y$ 을 추정한다
- 딥러닝은 다층신경망을 사용해 데이터로부터 특정 패턴 $\theta$를 추정
  
##### 기대값이란  

- 데이터를 대표하는 통계량
- 다른 통계적 범함수를 계산하는데 사용

$$
\mathbb{E}_{\mathbf{x} \sim P(\mathbf{x})}[f(\mathbf{x})]=\int_{X} f(\mathbf{x}) P(\mathbf{x}) \mathrm{d} \mathbf{x}, \quad \mathbb{E}_{\mathbf{x} \sim P(\mathbf{x})}[f(\mathbf{x})]=\sum_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x}) P(\mathbf{x})
$$

- 분산
    $$
    \mathbb{V}(\mathbf{x})=\mathbb{E}_{\mathbf{x} \sim P(\mathbf{x})}\left[(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{2}\right]
    $$

- 첨도

    $$
    \text { Skewness }(\mathbf{x})=\mathbb{E}\left[\left(\frac{\mathbf{x}-\mathbb{E}[\mathbf{x}]}{\sqrt{\mathbb{V}(\mathbf{x})}}\right)^{3}\right]
    $$

- 공분산

    $$
    \operatorname{Cov}\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\mathbb{E}_{\mathbf{x}_{1}, \mathbf{x}_{2} \sim P\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)}\left[\left(\mathbf{x}_{1}-\mathbb{E}\left[\mathbf{x}_{1}\right]\right)\left(\mathbf{x}_{2}-\mathbb{E}\left[\mathbf{x}_{2}\right]\right)\right]
    $$


#### 몬테카를로 샘플링

- 기계학습의 대부분은 확률분포를 명시적으로 모를 떄가 대부분
- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링이 필요

    $$
    \mathbb{E}_{\mathbf{x} \sim P(\mathbf{x})}[f(\mathbf{x})] \approx \frac{1}{N} \sum_{i=1}^{N} f\left(\mathbf{x}^{(i)}\right), \quad \mathbf{x}^{(i)} \stackrel{\text { i.i.d. }}{\sim} P(\mathbf{x})
    $$

- 독립추출만 보장된다면 대수의 법칙에 의해 수렴성을 보장한다.  


## 피어세션 정리

역전파에 대하여 진중한 토의를 했다.  
개념은 이해하는데 아직도 정확히 갈피를 못잡았다.

## 학습 회고

그래도 여기까진 할만하다. 다음에 쓸 글이 정말 답이 없다.