---
layout: post
title: 슬랙 팁 아카이빙    
category: BC
tag: []
---

## Tip

#### Python

- [파이썬 유용한 기본 라이브러리](https://wikidocs.net/book/5445)  
- [PEP란?](https://wikidocs.net/21733)
- [Pandas 프로파일링](https://wikidocs.net/47193)
- [Library dependency 확인](https://pypi.org/project/pipdeptree/)
- [Python Class](https://zzsza.github.io/development/2020/07/05/python-class/)


#### AI Math 

- [딥러닝 기초](https://ratsgo.github.io/statistics/2017/09/21/prob/)
- [최대 우도법](https://angeloyeo.github.io/2020/07/17/MLE.html)  
- [왜 CE를 쓸까?](https://theeluwin.postype.com/post/6080524)
- [최적화 알고리즘 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)  
- [Optimizer 정리(34페이지부터)](https://www.slideshare.net/yongho/ss-79607172)
- Convolution 연산 정리 [1](https://arxiv.org/pdf/1603.07285v1.pdf) [1_github](https://github.com/vdumoulin/conv_arithmetic) [2](https://poloclub.github.io/cnn-explainer/)
- [다양한 Scheduler](https://dacon.io/competitions/official/235697/codeshare/2373?page=1&dtype=recent)
- [Neural Network 내부 연산](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)


#### Pytorch
- [성능 향상 팁들](https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259)
- [다양한 Optimizer 구현](https://github.com/jettify/pytorch-optimizer)  



#### NLP

- [transformer의 positional encoding](https://inmoonlight.github.io/2020/01/26/Positional-Encoding/)
- [Huggingface Transformers 라이브러리에 대한 튜토리얼](https://eliza-dukim.tistory.com/20)
- [log softmax가 효율적인 이유](https://stackoverflow.com/questions/61567597/how-is-log-softmax-implemented-to-compute-its-value-and-gradient-with-better)



#### CV

- [metric: mAP](https://ctkim.tistory.com/79)  
- [metric: FPS](https://89douner.tistory.com/80)



#### Other Task
- [한국어 음성 데이터셋](https://aihub.or.kr/aidata/105)



#### Papers
- [AI는 특정 인종에 bias되어있다](https://arxiv.org/abs/2107.10356)
- [2021 State of AI Reports](https://www.stateof.ai/?fbclid=IwAR0otSZaIw72jKoBidDCI4X_Z_zxhVCAqBjCxSkS_aVY3tfdwsEMwtWdMUw)
- [GIRAFFE : 3D view renderings](https://openaccess.thecvf.com/content/CVPR2021/papers/Niemeyer_GIRAFFE_Representing_Scenes_As_Compositional_Generative_Neural_Feature_Fields_CVPR_2021_paper.pdf)
  - Paper full name : GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields
	- 기존의 deep generative models이 high resolution의 이미지도 굉장히 realistic하게 만들어내었지만 대부분의 모델이 2D 기반이기 때문에 생성하고자 하는 대상에 대한 controllability가 떨어진다.
	- NeRF 이후로 3D view rendering이 주목받고 있다
	- GIRAFFE는 compositional 3D scene representation를 통해 개별 object와 background를 disentangle한다
- [Labels4Free : Segmentation Dataset의 Unsupervised manner](https://rameenabdal.github.io/Labels4Free/)
  - Paper full name: Labels4Free: Unsupervised Segmentation using StyleGAN
	- Pre-trained StyleGAN을 확장하여 생성된 이미지의 foreground/background를 unsupervised separation
	- StyleGAN이 생성한 features들이 다른 task로 확장할 만큼 많은 정보를 갖고 있다는 연구들이 나오고 있는데 이를 unsupervised segmentation으로 적용한 논문
	- 이를 활용하여 segmentation을 위한 high quality의 데이터셋을 unsupervised manner로 생성할 수 있다
- [dataset distillation : 큰 데이터셋의 정보를 작은 데이터셋으로](https://arxiv.org/abs/2107.13034)
  - Knowledge distillation이 큰 모델의 정보를 작은 모델에게 전달해주는 것이라면, dataset distillation은 큰 데이터셋의 정보를 작은 데이터셋으로 전달해주는 기법
  - 해당 논문에서는 CIFAR-10의 0.02%인 10개의 데이터만 이용하여 64%의 test acc. 달성
  - 새로운 distributed kernel based meta-learning framework를 제안하여 해당 목표 달성
	- 아래의 그림은 논문에서 제안한 KIP라는 방법으로 이미지를 변환하였을 때의 결과이며 사람의 눈으로 보았을 때는 정확히 식별하기 어렵지만 모델의 성능을 대폭 향상
- [Swin Transformer](https://arxiv.org/abs/2103.14030)
- [An Empirical Study of Training Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.02057)
- [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)
- [Group-Free 3D Object Detection via Transformers](https://arxiv.org/abs/2104.00678)
- [Spatial-Temporal Transformer for Dynamic Scene Graph Generation](https://arxiv.org/abs/2107.12309)
- [Rethinking and Improving Relative Position Encoding for Vision Transformer](https://arxiv.org/abs/2107.14222)
- [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)
- [Learning Spatio-Temporal Transformer for Visual Tracking](https://arxiv.org/abs/2103.17154)
- [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/abs/2101.07448)
- [Vision Transformer with Progressive Sampling](https://github.com/yuexy/PS-ViT)
- [Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet](https://arxiv.org/abs/2101.11986)
- [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)
- [The Right to Talk: An Audio-Visual Transformer Approach](https://arxiv.org/abs/2108.03256)
- [Joint Inductive and Transductive Learning for Video Object Segmentation](https://arxiv.org/abs/2108.03679)
- [Conformer: Local Features Coupling Global Representations for Visual Recognition](https://arxiv.org/abs/2105.03889)
- [Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer](https://arxiv.org/abs/2108.03032)
- [Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://arxiv.org/abs/2108.03798)
- [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152)
- [MUSIQ: Multi-scale Image Quality Transformer](https://arxiv.org/abs/2108.05997)
- [SOTR: Segmenting Objects with Transformers](https://arxiv.org/abs/2108.06747)
- [Complementary Patch for Weakly Supervised Semantic Segmentation](https://arxiv.org/pdf/2108.03852.pdf)
  - Pixel-wise mask 대신 image-level tag만 이용하여 semantic segmentation을 수행하는 weakly-supervised 방법론
  - CAM을 활용한 기존의 방법들은 가장 discriminate한 object의 일부분만 잡아내어 정확도가 떨어짐
  - 해당 논문의 방법론을 따르면, 하나의 이미지에 대해 서로 complementary 관계에 있는 hidden patched parts에 대한 CAM 결과들을 더하면 훨씬 정확한 mask를 만들어낼 수 있음
- [YOLOF](https://arxiv.org/abs/2103.09460)
  - Detector의 핵심인 FPN이 좋은 성능이 보이는 이유가 multi-scale feature fusion 때문이 아니라 각 level의 feature를 별개로 고려하는 divide-and-conquer 때문이라는 것을 밝혀냄
  - Divide-and-conquer가 좋은 성능을 유도하는 것은 맞지만 memory burben이 커서 inefficient
  - 해당 논문에서는 dilated encoder와 uniform matching이라는 두가지 기법을 통해 single-level의 feature map만 이용함에도 불구하고 RetinaNet보다 약 2.5배 빠르고 성능이 더 높은 YOLOF 모델 제안
  - Idea: YOLOF가 single-level feature map만으로도 좋은 detection 성능을 달성하였는데 이를 transformer와 활용하면 훨씬 효율적인 attention 기반의 detector가 만들어지지 않을까요?
- [Few-Shot Object Detection via Classification Refinement and Distractor Retreatment](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Few-Shot_Object_Detection_via_Classification_Refinement_and_Distractor_Retreatment_CVPR_2021_paper.pdf) 
  - Few-shot object detection의 evaluation metric인 Average Precision (AP)가 class와 box quality를 동시에 고려하는데 대부분의 정량적 성능 저하가 classfication error에서 온다는 것을 밝힘 (즉, box quality는 좋은데 misclassification하는 문제가 주된 요인)
  - Architecture-level enhancement: 새로운 few-shot correction network로 category confusion을 줄임
  - 불완전한 annotation을 갖고 있어서 성능을 대폭 줄이는 data samples을 distractor로 정의하고 해당 distractor를 제거하고 semi-supervised loss를 통해 활용하는 방법 제안
- [Points as Queries: Weakly Semi-supervised Object Detection by Points](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Points_As_Queries_Weakly_Semi-Supervised_Object_Detection_by_Points_CVPR_2021_paper.pdf)
  - Object detection의 성능을 높이기 위해 bounding box보다는 coarse한 annotation인 point 정보만 존재하는 weakly-supervised extra data를 활용하여 semi-supervised learning을 진행하는 weakly-semi-supervised object detection 방법론 제안
  - Fully-supervised teacher model를 inference하여 만들어진 pseudo-labels을 바탕으로 student network 학습
  - DETR을 확장하여 이미지 정보는 encoder로 추출하고 point 정보는 encoding하여 decoder의 query로 넣어주는 Point DETR 제안
- [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers ](https://arxiv.org/abs/2106.10270)
  - ViT를 어떻게 효율적으로 학습하고 주어진 데이터셋에서 좋은 성능을 달성할 수 있을지를 분석한 논문
  - 다양한 데이터셋에 학습된 무려 [총 50,000개의 ViT 모델 공개](https://github.com/google-research/vision_transformer)
  - 요약 1) 데이터 수가 적은 경우에는 augmentation과 regularization이 굉장히 중요
  - 요약 2) 데이터 수가 많을 때에는 aug./reg.를 크게 신경쓰지 않아도 좋은 성능
  - 요약 3) Pre-trained weights로 fine-tuning하는 것이 성능 향상에 도움
- [End-to-End Semi-Supervised Object Detection with Soft Teacher](https://arxiv.org/abs/2106.09018) 
  - Unlabeled dataset을 추가로 활용하여 성능을 높이는 semi-supervised 방법론 제안
  - Multi-stage training이 필요한 기존 방법과 달리 end-to-end 학습
  - 학습을 진행하면서 점점 정확한 psudeo label을 만들어가는 방식
  - 크게 unlabeled data에 대한 classification loss와 regression loss를 구분하여 학습
  - 1) Classfication head: soft teacher에서 만든 prediction에 대해 score filtering을 진행하여 얻은 box에 대해서만 loss 계산
  - 2) Regression head: regression variance가 낮은 box만 filtering하여 해당 box에 대해서만 loss 계산
- [Revisiting Mask-Head Architectures for Novel Class Instance Segmentation](http://ai.googleblog.com/2021/09/revisiting-mask-head-architectures-for.html)
  - Instance Segmentation은 novel class에 대한 예측이 어려움(마스크를 그리기 어렵기 때문에)
  - protocol과 mask-head 구조를 바꿔서 supervised learning과의 성능 차이를 좁힘
  - Crop을 custom하게 한 처음 보는 물체에 대하여 잘 작동하는 novelty가 보임.
  - VOC에서 4.7% 상승한 mask mAP sota 성능 (no auxiliary loss functions, offline trained priors, weight transfer functions)
  - Deep-MAC이라는 오픈소스로 공개
- [3DETR](https://arxiv.org/abs/2109.08141)
  - Facebook에서 이제 3D object deteciton까지 transformer로 하기 시작했습니다!..
  - input point cloud로부터 transformer encoder가 feature를 얻어내고 decoder에서 box를 prediction
  - 주어진 reference point에 해당하는 query embedding을 받은 decoder는 이와 관련된 points를 attention하여 detection 성능 향상
- [Reconcile Prediction Consistency for Balanced Object Detection](https://arxiv.org/abs/2108.10809) 
  - 기존의 detector는 classification loss와 regression loss를 완전히 독립적으로 학습하기 때문에 많은 inconsistent predictions 유발 (예를 들면, classification score는 높지만 localization acc.는 낮은 경우)
  - Prediction consistency를 위하여 Harmonic loss라는 새로운 loss 제안
  - classification branch와 localization branch의 optimization을 harmonize하는 역할
- [Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.pdf) 
  - Figure 2에 나온 것처럼, 단순히 특정 이미지의 object를 복사(copy)한 다음 변형(random scale jittering)을 가하고 다른 이미지에 붙여 넣는 (paste) 굉장히 simple한 augmentation이 instance segmentation의 성능을 올려준다
  - cutmix의 instance segmentaton 느낌
- [StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis](https://openreview.net/forum?id=iUuzzTMUw9K)
  - NeRF (ECCV 2020) 이후로 neural radiance fields를 이용하여 여러 각도에서 바라본 이미지를 생성하는 task가 굉장한 주목을 받고 있습니다.
  - 이러한 NeRF에 StyleGAN의 컨셉을 넣어 3D 영상을 생성함과 동시에 style attribute 또한 control 가능한 StyleNeRF 제안
- [Patches Are All You Need?](https://openreview.net/pdf?id=TVHS5Y4dNvM)
  - 개인적으로 정말 흥미롭게 읽은 논문이고, 특히 ViT에 관심이 있으신 분들이라면 더욱 재밌게 읽으실 수 있을 것 같습니다 :미소짓는_얼굴:
  - ViT의 특징을 크게 2가지로 구분한다면, ‘(1) 이미 그 자체로 강력한 Transformer의 활용’ 과 ‘(2) input을 patch단위로 쪼개어 활용하는 것’으로 나눌 수 있다.
  - 저자는 (1)과 (2) 중에서 정말 어떤 것이 ViT의 높은 성능에 기여했는지를 알고 싶어 하였고, 그 결과 놀랍게도 (2) patch 단위로 입력을 쪼개어 넣어주는 것이 성능에 많은 영향을 미친다는 것을 밝혀내었다.
  - 이러한 발견을 기반으로, 입력을 patch 단위로 쪼개어 넣되 ViT보다 훨씬 가볍고 효율적이며 심플한 ConvMixer라는 모델을 제안하였고 해당 모델은 오직 convolution만 사용.
  - ConvMixer는 ViT와 ResNet보다 parameter 수는 적지만 더욱 높은 성능 달성
  - 결론) patch embedding을 활용하는 것은 NLP에서 tokenization을 사용하는 것처럼 CV에서도 굉장히 중요하다!
- [Audio-Guided Image Manipulation for Artistic Paintings](https://kuai-lab.github.io/aiartist/)
  - 오디오를 기반으로 이미지를 Manipulation 합니다.
  - CLIP embedding space에 오디오를 align하였습니다
  - StyleGAN의 latent code를 오디오로 guide하여 소리의 의미에 맞게 생성하도록 합니다.
- [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178v1) 
  - CNN은 (장점) inductive bias를 통해 적은 parameter로도 좋은 representation을 얻을 수 있지만, (단점) spatially local하다.
  - ViT은 (장점) global representation을 얻을 수 있지만, (단점) heavy-weight하다.
  - Idea: CNN과 ViT의 장점을 결합하여 가볍고 빠른 모델을 만들 수 있지 않을까?
  - Standard conv.의 3가지 operations (unfolding, local processing, folding) 중에서 local processing 과정을 transformer를 활용한 global processing으로 대체하여 CNN과 ViT 각각의 장점만 활용
  - 기존 conv. 기반의 MobileNet 계열보다 적은 수의 parameter로 훨씬 높은 성능을 보였고, 심지어 ResNet-101의 약 9분의 1 크기의 모델로 더 높은 성능 달성
- [BEiT: BERT Pre-Training of Image Transformers](https://openreview.net/forum?id=p-BhZSz59o4)
  - NLP의 BERT를 따라 image transformer에도 pre-training 방법 제시
  - BEiT (Bidirectional Encoder from Image Transformers)
  - 이미지를 visual tokens으로 tokenize한 다음, 일부 patch를 masking하고 transfomer에 넣은 뒤 corrupted image patch를 복원하는 것을 pre-training의 objection로 설정
  - Image transformer에도 BERT처럼 pre-training model을 기본적으로 꼭 사용하는 날이 머지않아 올지도 모르겠네요
- [Towards Real-World Blind Face Restoration with Generative Facial Prior](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_Real-World_Blind_Face_Restoration_With_Generative_Facial_Prior_CVPR_2021_paper.pdf)
  - Low quality의 face image를 high-quality로 향상시키는 blind face restoration task
  - low quality face는 정확한 geometric 정보를 담고있지 못한다는 문제를 가지고 있는데 이를 pretrained face GAN이 담고 있는 다양한 prior를 이용하여 해결
  - Network 구성: (1) Degradation removal 역할의 U-Net / (2) facial details를 채워주는 Pretrained GAN as prior
  - Take-home message: 단순히 새로운 혹은 높은 성능의 GAN을 만드는 것이 아닌 기존에 학습된 GAN을 pre-trained model로 사용하여 다양한 task에 확장하는 시도들이 늘어나고 있는데, blind face restoration에도 효과적으로 작용하며 pre-trained GAN은 유용한 prior를 많이 가지고 있다
- [instance segmentation](https://arxiv.org/abs/2008.10032)
  - class imbalace가 학습을 어떻게 방해하는지 그 원인을 분석하고, data agnostic 하지 않게 (data의 statictics를 사용하지 않고) 문제를 해결
- [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)
  - Batch size의 영향, Learning rate scheduling (warm-up, cosine lr decay), Batch normalization initialization, No bias decay, Low-precision training, Model tweaks, Label smoothing, Knowledge distillation, Mix-up training, Transfer learning

- [논문 정리하시는분 레포](https://github.com/kweonwooj/papers)



#### MLOps

- [유용한 논문](https://cloud.google.com/resources/mlops-whitepaper)
- [Uber의 데이터 파이프라인](https://eng.uber.com/michelangelo-machine-learning-platform/)
- [MLOps 개론](https://ml-ops.org/)




#### 시각화
- [BatchNorm 시각화](https://github.com/tomgoldstein/loss-landscape)
- [Moments of Happiness](https://flowingdata.com/2021/07/29/counting-happiness/)
- [시각화에서 데이터 유실될 때](https://herjh0405.tistory.com/124)
- [Pandas 시각화](https://www.kaggle.com/subinium/all-you-need-is-pandas-benchmark-viz)
- [Seaborn 실습](https://www.kaggle.com/learn/data-visualization)
- [다양한 시각화 Article](https://uxdesign.cc/20-ideas-for-better-data-visualization-73f7e3c2782d)
- [Conv Layer 깊이 시각화](https://ezyang.github.io/convolution-visualizer/index.html)
- [Bar chart race](https://github.com/dexplo/bar_chart_race)



#### Linux

- [GPU 모니터링](https://github.com/subinium/Misc-Cheatsheet/blob/master/linux/gpu.md)
- [명령어 설명](https://explainshell.com/)




#### Git

- [Git Flow](https://techblog.woowahan.com/2553/)
- [Git Message Tip](https://meetup.toast.com/posts/106)  




#### LoadMap

- [AI Expert Roadmap](https://i.am.ai/roadmap/#note)
- [data scientist, data engineer, machine learning](https://github.com/AMAI-GmbH/AI-Expert-Roadmap)
- [machine learning mindmap](https://github.com/dformoso/machine-learning-mindmap) [loadmap](https://github.com/mrdbourke/machine-learning-roadmap) [other](https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva)
- [Deep learning](https://github.com/instillai/deep-learning-roadmap)
- [Deep learning papers](https://tensorflow.blog/2016/10/25/deep-learning-papers-reading-roadmap/) / [2](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)
- [fullstack deeplearning](https://fullstackdeeplearning.com/spring2021/lecture-6/)
- [ML pipeline](https://github.com/graviraja/MLOps-Basics)
- [ai-infrastructures](https://github.com/1duo/awesome-ai-infrastructures)



#### ETC

- [LaTeX 기호 모음](https://jjycjnmath.tistory.com/117)
- [OCR 기반 LaTeX 문법 생성기](https://mathpix.com/)
- [LaTeX 그려서 찾기](http://detexify.kirelabs.org/classify.html)
- [Large-scale LM(multi gpu)](https://www.youtube.com/watch?v=w4a-ARCEiqU&t=1978s)
- [VSCode zsh 사용하기](https://l-yohai.github.io/VSCode-Terminal-Customizing/#more)
- [IOC Container Pattern](https://aitech2.slack.com/archives/C02PD4MNYVC/p1639812519397700?thread_ts=1639633874.343500&cid=C02PD4MNYVC)
- [안수빈님 치트시트](https://github.com/subinium/Misc-Cheatsheet)
- [PDF 용량 줄이기](https://askubuntu.com/questions/113544/how-can-i-reduce-the-file-size-of-a-scanned-pdf-file/256449#256449)
- [서빙 레포](https://github.com/zzsza/Boostcamp-AI-Tech-Product-Serving)
- [Jupyter lab extension](https://github.com/markusschanta/awesome-jupyter#jupyterlab-extensions)
- [VPC란?](https://medium.com/harrythegreat/aws-%EA%B0%80%EC%9E%A5%EC%89%BD%EA%B2%8C-vpc-%EA%B0%9C%EB%85%90%EC%9E%A1%EA%B8%B0-71eef95a7098)
- [OOP](https://zzsza.github.io/development/2018/09/09/oop/)
- [Specialist? Generalist?](https://hbr.org/2019/03/why-data-science-teams-need-generalists-not-specialists)
- [데이터분야 커리어 회고본](https://dailyheumsi.tistory.com/168?category=864097)
- [회고본2](https://www.humphreyahn.dev/blog/early-2020-review)
- [데이터사이언티스트에 대하여](https://zzsza.github.io/diary/2019/04/05/how-to-study-datascience/)