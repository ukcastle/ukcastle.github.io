---
layout: post
title: Deep Learning Basics - CNN의 발전
category: BC
tag: [Deep Learning] 
use_math: true
---

## Convolution이란?

- 합성곱 연산자
- 행렬 연산으로 딥러닝 영역에서 매우 자주 사용된다.  
- 이미지에 NxN 크기의 필터를 적용시켜주는 것을 Convolution 연산이라고 한다.  

#### Convolutional Neural Networks

- Convolution Layer + Pooling Layer  
    네트워크의 초반부에서 이미지의 유용한 정보를 추출하기 위해 반복적으로 사용됨  
- Fully Connected Layer  
    네트워크의 후반부에서 최종적으로 이미지를 결정하고 분류

> 레이어를 깊게 설계하며 parameter 숫자를 줄이는 방향으로 발전했다.  

- Stride  
  연산을 할 때 건너뛰는 숫자를 의미, 늘어날수록 Layer의 숫자가 줄어들며 굳건해진다.  
- Padding  
  테두리부분을 0으로 채워줘서 연산을 하면서 레이어의 크기가 줄어드는것을 방지

- 1x1 Convolution  
  채널을 감소하는 효과 (256x256x128 * 1x1x128x32 -> 256x256x32)  
  레이어를 깊게 쌓으며 parameter을 줄이는 테크닉으로 자주 사용된다.  
  

## Modern CNN - 1x1 Convolution의 중요성  

#### AlexNet

- 딥러닝이 주목받기 시작된 시기
- 당시 GPU의 한계로 지금 보면 갸우뚱한 선택들이 좀 있다.  
- 하지만 ReLU, Overlapping Polling, Data Agumentation, Dropout 등 현재도 많이 쓰이는 기술들을 채택한 네트워크이다.  
  
#### VGGNet

- 3x3(stride=1) 필터의 반복
- 16층, 19층의 레이어 수 까지 발전    
- 예를들면 3x3을 두번 중첩시켜 같은 효과를 내지만 파라미터 수는 60%까지 줄이는 장점


#### GoogLeNet

- 1x1 Convolution의 등장
- 각 층에 Inception Block을 생성  
- 1x1 Convolution을 사용해 채널 방향으로 Dimension 크기를 줄이는 효과가 있다  
- Dimension 크기를 줄여 파라미터 수를 줄이는 효과
- 3x3x128x128 -> 1x1x128x32+3x3x32x128 (약 70퍼센트 감소 효과)  

#### ResNet

- ResNet 이전까진, 레이어의 수가 너무 많아지면 Overfitting이 되는 경향이 있었다.  
- Skip-Connection (비선형 활성함수 뒤 Identity Map를 더해줌)
- Parameter을 줄이고 성능을 늘리는 네트워크를 깊게 쌓을 수 있게됨

#### DenseNet

- Concatenation  
- ResNet과 비슷한데, DenseNet은 IdentityMap을 **Concat** 해줌
- 크기를 늘리다가 BatchNormalize를 통해 압축해줌
- 결과값을 축적

## Computer Vision Applications

- Semantic Segmentation  
  이미지에서 사물을 나누는 기술  
  자율주행에서 많이 사용된다  
- Fully Convolutional Network  
  기존 CNN의 Dense 레이어를 없애고, 전체 이미지를 하나의 히트맵으로 만든다.  
  파라미터 수에는 변함이 없고 한 줄로 쫙 펴준다고 생각하면 편하다.  
- Deconvolution  
  원리는 모든 픽셀을 그대로 연산하긴 무리가 있어 줄인 다음 연산을 한 뒤 다시 up-sampling을 하는 방식이다.  
  사실 완벽한 역연산은 불가능하다. stride와 padding을 잘 참조해서 만든다.  

#### Detection

- R-CNN  
  가장 간단한 CNN으로 약 2000개의 선택영역을 만들고 각각을 모두 AlexNet으로 연산을 한 뒤 SVM으로 분류를 한다. 각각을 모두 연산함에 따라 매우 오래걸리는 단점이 있다.  
- SPPNet  
  이미지가 있다 생각되는 부분에 단 한번만 CNN 연산을 함으로써 적은 연산 수를 보증해준다.  
- Fast R-CNN  
- Region Proposal Network
- Faster R-CNN
- YOLO  
  Bounding Box를 만드는 연산을 먼저 하지 않고 물체가 있는 셀을 분리한 후 그를 기반으로 Bouding Box와 Class를 찾는 연산을 같이 해서 매우 빠르다.  


## 피어세션 정리  

[CNN의 시각적인 설명](https://poloclub.github.io/cnn-explainer/)
[Receptive Field란?](https://itrepo.tistory.com/32)