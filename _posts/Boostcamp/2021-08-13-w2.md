---
layout: post
title: Deep Learning Basics
category: BC
tag: [Deep Learning] 
use_math: true
---

## 주간 정리 내용

[1일차](https://ukcastle.github.io/bc/2021/08/09/w2d1/)  
[2일차](https://ukcastle.github.io/bc/2021/08/10/w2d2/)  
[3일차](https://ukcastle.github.io/bc/2021/08/11/w2d3/)  
[4일차](https://ukcastle.github.io/bc/2021/08/12/w2d4/)  
[5일차](https://ukcastle.github.io/bc/2021/08/13/w2d5/)  

## 과제 수행 내용

- ViT : 한글자 잘못써서 계속 안됐었는데, 완성했다
- AAE : 힌트를 너무 잘 주셔서 성공했다  
- MDN : 개념은 이해했는데.. 잘 모르겠다  

## 피어세션 정리

[휴라스틱 이론이란?](https://ko.wikipedia.org/wiki/%ED%9C%B4%EB%A6%AC%EC%8A%A4%ED%8B%B1_%EC%9D%B4%EB%A1%A0)   
[PyTorch의 todevice() 메서드에 관해](https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor)  
[PyTorch의 DataLoader의 workers에 관해](https://jybaek.tistory.com/799)  
[활성함수를 고르는 척도](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)  
[Optimizer 총 정리](https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html)  
[MLE = MSE인건가?](https://www.jessicayung.com/mse-as-maximum-likelihood/)  

[추후 참고할만한 1기 캠퍼님 블로그](https://philgineer.github.io)  
[k-validation](https://nonmeyet.tistory.com/entry/KFold-Cross-Validation교차검증-정의-및-설명)  
[Attention Is All You Need](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  
[RMS Prop, Adam](https://light-tree.tistory.com/141)  
[RAdam](https://zzaebok.github.io/deep_learning/RAdam/)  
[MDN 한글번역](https://kangbk0120.github.io/articles/2018-05/MDN)  
[k-validation k값 정하는 척도](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)  
[부스팅과 앙상블](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%[…]A%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting)   

[CNN의 시각적인 설명](https://poloclub.github.io/cnn-explainer/)  
[Receptive Field란?](https://itrepo.tistory.com/32)  
[CNN에 대하여](https://89douner.tistory.com/57)  

[positional encoding이란?](https://skyjwoo.tistory.com/entry/positional-encoding이란-무엇인가)
[NN에 대한 시각적인 이해](https://www.youtube.com/watch?v=aircAruvnKk)  
[Word Embedding이란?](https://wikidocs.net/22660)  
A. positional embedding이 x의 값과 상관이 있는 것인지 아니면 x의 값과 상관 없이 위치만 고려해서 embedding이 되는 것인지?  
A. query, key, value의 값을 어떻게 정하는지, 정하는 모델이 있는지, 있다면 그 모델의 파라미터 값도 같이 학습을 하는지?  

[ViT에 대한 해설](https://engineer-mole.tistory.com/133)  
A. ViT를 봤을때, Transformer의 비전은  
Q. 상당히 강력하다.  [Multi-modal](https://arxiv.org/abs/2104.11178v1?fbclid=IwAR0TMyz-tAmgqBqBD7WZ2icUZWne9t5d73vmypC6Ts7oadxHha-ltMjK3m8), [VATT](https://arxiv.org/pdf/2102.02779.pdf)를 참고하면 좋다. 그렇다고 CNN RNN보다 무조건 Powerful하냐고 하면 [아니다.](https://arxiv.org/pdf/2106.04554.pdf)  
[Parameter 수와 학습 결과에 대하여 연구한 최신 논문(교수님 강추)](https://arxiv.org/pdf/2001.08361.pdf)  
<br>
질문에 대해 많은곳에서 알아봐주신 멘토님 감사해요!!  


## 회고

저번주까진 딥러닝은 무엇일까? 라는 질문에 대해서는 상당히 추상적으로 생각했다.  
이번주의 수업을 듣고보니 연속된 층(Neural network)을 이용해 학습하여 향상된 추론 결과를 도출해내는 방식이라고 생각한다.  
물론 아직도 잘 모른다.  
그래도 교수님들과 멘토님들덕에 좋은 방향으로 한 발자국 전진했다고 생각한다.<br>  
또 느낀점은 주 레퍼런스로 논문을 상당히 많이 참조한다고 생각했다. CE분야에서는 지식 베이스 자체가 깊지않고, 굳이 구현하는데 논문까지..? 라는 생각도 있었는데 ML/DL분야에서는 논문을 읽고 검증하는 과정이 필수라는 것을 느꼈다.  
당장 내가 졸업작품에서 썼던 유명한 오픈소스 라이브러리도 논문으로 나온거였더라...  
물론 글을 읽고 책을 읽는것을 좋아하고 쓰는것도 좋아한다. 당장 지금 이 포스팅을 끝내고도 책을 읽으러 갈 예정이다.  
근데 논문을 읽는것은 학부연구생을 하면서도 항상 느끼지만, 어렵다.  
대부분 IEEE급 논문을 읽어서 그런가, 영단어 자체가 이해가 안될때도 많고 영어 자체도 어려워서 한 페이지를 읽는데도 수 시간이 걸릴때도 있다. 그래도 이런 경험이 쌓이고 쌓이면 추후 영어실력도 일취월장하지 않을까.  
