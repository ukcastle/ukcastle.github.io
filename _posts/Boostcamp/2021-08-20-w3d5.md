---
layout: post
title: PyTorch - Pytorch etc
category: BC
tag: [Deep Learning] 
use_math: true
---

## DataParallel

Multi-GPU를 사용할 때는 특별한 작업이 필요하다.  

```py
parallel_model = torch.nn.DataParallel(model)

# Forward
predictions = parallel_model(inputs)

# Compute
loss = loss_function(predictions, labels)

# Average
loss.mean().backward() # 여러 GPU에서 받은 결과를 합친다  
optimizer.step()

# Forward pass with new parameters
predictions = parallel_model(inputs)

```

이 방법 외에도 `DistributedDataParallel` 모듈을 이용할 수도 있다.  

## Hyperparameter Tuning

이미 정확도가 충분히 올라왔을때, 1퍼센트라도 더 올리고 싶을 경우 사용한다.  

#### Ray

- Multi-node multi processing 지원 모듈
- ML/DL 병렬 처리를 위한 모듈  
- Hyperparameter Search를 위한 다양한 모듈을 제공  

## OOM Troubleshooting  

**OOM**이 발생할 시 여러 문제점 때문에, 원인을 파악하기 힘들 때가 있다.  

일단, Batch Size를 줄여보고(극단적으로 1로도 줄여보자) GPU를 clean 해준 뒤 다시 돌려보자.  

그 다음, `GPUtil` 라이브러리의 `GPUtil.showUtilization()` 메서드를 이용하여 모니터링도 해보자.  

GPU Clean을 할 땐 `torch.cuda.empty_cache()`를 사용하자.   