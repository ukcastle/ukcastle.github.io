---
layout: post
title: Computer Vision - Efficient Learning
category: BC
tag: [Deep Learning]
---

## Data Augmentation  

CNN은 데이터를 압축하여 가중치를 만드는 작업  
모든 이미지는 저마다의 bias가 있어, 각 클래스마다 특정한 패턴은 있지만 그 외에는 여러가지 bias들이 있다.  
따라서 여러 변수를 고려해야 Over-fitting되지 않는 모델을 만들 수 있다.  
따라서 Data Augmentation이란 주어진 한정된 데이터속에서 여러가지 bias를 넣어줘 일반화를 시켜주는 강력한 방법이다.  

수업에는 나오지 않았지만, [Albumentation](https://github.com/albumentations-team/albumentations)라이브러리를 상당히 잘 썼다.  

Cut-Mix와 같은 방법도 효과적이라고 한다. 예전 프로젝트에서는 썼을때 마땅한 성능 상승이 없어서 사용하지 않았던 기억이 있다. 다음엔 좀 더 신중하게 사용해봐야겠다.  

## Transfer Learning

모델을 Zero-Base로부터 만들 때는 상당히 많은 비용이 들어간다.  
이런 경우를 대비하여 **다른 데이터셋에서 배운** 지식을 현재의 모델의 특징으로 적용시키는 방법을 사용한다.  

fc레이어만 잘라내고 상황에 맞는 새로운 fc레이어를 만든 뒤 업데이트시키는 방법을 사용한다.  

Conv레이어 또한 적은 lr로 학습시키는 Fine-tuning 방법도 있는데, 시간은 더 오래걸리지만 더 강력할 수도 있다.  

#### Knowledge distillation  

이미 학습된 Teacher Network 모델로 더 적은 모델인 Student Model을 훈련시키는 방법이다.  
요 근래에는 학습된 Student에 Teacher의 데이터까지 훈련시켜 더 큰 네트워크를 만드는 방법이 효과적이라는 연구가 있다.  

동일한 input X가 있을 때 Teacher의 Output과 Student의 Output에 대하여 KL div Loss를 사용해 **Student**만을 업데이트하는 방식이다.  
이런 방식으로 Label을 제공해주지 않는 방법으로 Unsupervised Learning이라고 할 수도 있다.  

이 경우 softmax 함수에 Temperature을 적용시켜 smooth한 Loss를 사용한다. 즉 두 가지 값의 차이를 측정하는 KLdiv(Soft label, Soft prediction) 으로 두개의 차이를 비교하고 Student를 업데이트한다.  
이 후 Student에는 CrossEntropy(Hard label, Soft prediction)으로 올바른 정답을 찾는 학습을 한다.  

결국 Student는 KLdiv loss와 CE loss 두 가지로 학습을 진행한다.  
 
## Unlabled Data  

Unlabled 데이터의 특징은 라벨링 없이 상당히 많이 사용할 수 있다는 것이다.  

큰 방법론은 Unlabeled Data를 라벨링 된 데이터로 이미 훈련된 Teacher 모델로 pseudo-labeled Data로 만들고 라벨링 된 데이터와 pseudo-labeled 데이터를 합쳐 Student 모델을 만든다.  

그 다음 Student 모델을 다시 Teacher로 올리고 Unlabeled 데이터의 훈련을 반복하는 방법이다.  

흥미로운점은 매 훈련때마다 **더 큰 Student 모델을 사용**하는 방법이다.  