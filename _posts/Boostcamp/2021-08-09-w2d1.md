---
layout: post
title: Deep Learning Basics - Overview
category: BC
tag: [Deep Learning] 
use_math: true
---

## Introduction

#### 딥러닝의 4가지 핵심 요소

1. 모델을 학습시킬 수 있는 **데이터**
2. 데이터를 어떻게 변환시킬지 결정해주는 **모델**
3. 모델의 성능을 수량화해주는 **손실함수**
4. 손실을 최소화시키는 변수를 조정하는 **알고리즘**

#### 딥러닝의 주요 아이디어들

본 자료는 "Deep Learning's Most Important Ideas - A Brief Historical Review, Denny Britz, 07-29-2020" 에 기반하였음

- AlexNet - 2012
    
    CNN 기반의 네트워크로 구성되어있다.  
    처음으로 머신러닝 분야에서 딥러닝 네트워크가 성능이 더 좋게 나온 역사적인 네트워크이다.

- DQN - 2013

    알파고로 유명한 네트워크

- Encoder / Decoder - 2014

    번역 시스템에서 자주 보이는 네트워크

- Adam Optimizer - 2014

    상당히 많은 부분에서 안정적인 최적화가 가능한 방법

- Generative Adversarial Network - 2015

    GAN 이라고 불리며 이미지 인식쪽에서 상당히 많이 사용된다.

- Residual Network - 2015

    ResNet 이라고 불리며 상당히 많은 Convolution 레이어를 쌓아 올린 네트워크이다.

- Transformer - 2017

    Encoder / Decoder 구조로 설계되었음에도 RNN보다 좋은 성능을 보이며 NLP에서 상당히 많은 부분에서 사용되는 기술이다.

- BERT - 2018

    Transformer 기반으로 설계되어 특정한 언어를 처리하기 전 크게 관련이 없는 문서에서 Pre-Training을 한 뒤 Fine-Tuning으로 모델을 튜닝시키는 방법이다. 

- BIG Language Models - 2019
- Self Supervised Learning - 2020

## Neural Network & Multi-Layer Perceptron

#### Neural Network

초기의 설계는 인간의 뇌를 모델링한다고 생각했다. 하지만, 추후 발전해가면서 딥러닝의 네트워크는, **비선형 연산이 반복적으로 일어나는 것**으로 정의를 내릴 수 있게 되었다.

- Linear Neural Networks

    가장 간단한 신경망을 예로 들어보자.  
    $y = Wx+b$의 형태에서 미분값에 가중치를 준 뒤 더하고, 손실함수를 계산한 뒤 활성함수를 씌워 비선형변환을 시켜주는 것을 반복한다.  
    
- 손실함수

    * MSE: 계산값과 예측값의 차이를 제곱한 뒤 전체로 나눔
    * CE: 실제 분포와 예측 분포 차이의 값을 계산
    * MLE: 모수적인 데이터 측정법
    
    상당히 가볍게 작성하였으며 각각이 어느 때 필요할 지 열심히 공부해야 한다.

- 활성함수

    * Sigmoid
    * Tanh
    * ReLU

    딥러닝에서 활성함수가 없다면, 딥러닝이라 할 수 없다.  

## 피어세션 정리

[휴라스틱 이론이란?](https://ko.wikipedia.org/wiki/%ED%9C%B4%EB%A6%AC%EC%8A%A4%ED%8B%B1_%EC%9D%B4%EB%A1%A0)   
[PyTorch의 todevice() 메서드에 관해](https://stackoverflow.com/questions/63061779/pytorch-when-do-i-need-to-use-todevice-on-a-model-or-tensor)

[PyTorch의 DataLoader의 workers에 관해](https://jybaek.tistory.com/799)

[활성함수를 고르는 척도](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)

[Optimizer 총 정리](https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html)

[MLE = MSE ??](https://www.jessicayung.com/mse-as-maximum-likelihood/)


## 회고

피어세션을 하면 항상 생각한다. 나는 그냥 놓치고 갈 정보들을 상당히 많이 캐치한다.  
내가 주입식교육에 많이 빠져있는 것 같기도 하다. 이걸 고쳐야되는데.